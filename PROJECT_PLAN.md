# Project Plan: AI-Based Automated Essay Scoring System
## Research for Q1 Scopus Publication

---

## ðŸ“‹ Executive Summary

**Judul Penelitian:**  
"Comparative Analysis of Large Language Models (ChatGPT vs Gemini) for Automated Essay Scoring: A Multi-Criteria Assessment with Fleiss' Kappa Agreement"

**Tujuan:**
1. Mengembangkan sistem AES menggunakan ChatGPT dan Gemini
2. Mengukur konsistensi penilaian melalui 4x percobaan
3. Membandingkan akurasi dengan ground truth dosen
4. Menganalisis inter-rater agreement menggunakan Fleiss' Kappa

---

## ðŸŽ¯ Research Questions

1. **RQ1:** Seberapa konsisten ChatGPT dan Gemini dalam memberikan skor esai pada multiple trials?
2. **RQ2:** Bagaimana akurasi ChatGPT dan Gemini dibandingkan dengan penilaian dosen?
3. **RQ3:** Seberapa tinggi kesepakatan (agreement) antara AI models dan dosen menggunakan Fleiss' Kappa?
4. **RQ4:** Model mana yang lebih reliable untuk automated essay scoring?

---

## ðŸ“Š Metodologi Research

### A. Data Collection
- **Jumlah Soal:** 8 soal esai
- **Jumlah Mahasiswa:** 10 mahasiswa (80 esai total)
- **Rubrik:** 4 kriteria penilaian dengan skala A, B, C, D/E

### B. Eksperimen Design
```
Setiap Jawaban Mahasiswa â†’ Dinilai oleh:
â”œâ”€â”€ ChatGPT (4x percobaan)
â”œâ”€â”€ Gemini (4x percobaan)
â””â”€â”€ Dosen (1x sebagai ground truth)

Total: 10 mahasiswa Ã— 8 soal Ã— 4 trials Ã— 2 models = 640 API calls

Output per jawaban:
- 4 nilai per kriteria (total 16 nilai per model)
- Skor akhir weighted
- Consistency metrics
```

### C. Evaluation Metrics

#### 1. Consistency Metrics (4x Trials)
- Standard Deviation (SD)
- Coefficient of Variation (CV)
- Range (Max - Min)
- Intraclass Correlation Coefficient (ICC)

#### 2. Accuracy Metrics (vs Dosen)
- Accuracy (exact match)
- Mean Absolute Error (MAE)
- Root Mean Square Error (RMSE)
- Precision, Recall, F1-Score (per grade)
- Confusion Matrix

#### 3. Inter-Rater Agreement
- **Primary:** Fleiss' Kappa (ChatGPT + Gemini + Dosen)
- **Secondary:** Cohen's Kappa (pairwise)
  - ChatGPT vs Dosen
  - Gemini vs Dosen
  - ChatGPT vs Gemini

---

## ðŸ—‚ï¸ Project Structure

```
AES/
â”œâ”€â”€ ðŸ“ config/
â”‚   â”œâ”€â”€ rubrics.json              # Default & custom rubrics
â”‚   â”œâ”€â”€ models_config.yaml        # API keys, parameters
â”‚   â””â”€â”€ experiment_config.yaml    # Trials, batch size, etc.
â”‚
â”œâ”€â”€ ðŸ“ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ questions.csv         # Bank soal esai
â”‚   â”‚   â”œâ”€â”€ student_answers.csv   # Jawaban mahasiswa
â”‚   â”‚   â””â”€â”€ lecturer_scores.csv   # Ground truth dari dosen
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ dataset.json          # Unified dataset
â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ chatgpt_trials/       # 4x percobaan ChatGPT
â”‚       â”œâ”€â”€ gemini_trials/        # 4x percobaan Gemini
â”‚       â””â”€â”€ analysis/             # Statistical results
â”‚
â”œâ”€â”€ ðŸ“ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ðŸ“ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_agent.py         # Abstract class
â”‚   â”‚   â”œâ”€â”€ chatgpt_agent.py      # OpenAI implementation
â”‚   â”‚   â””â”€â”€ gemini_agent.py       # Google Gemini implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rubric.py             # Rubric management
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py     # Dynamic prompt generation
â”‚   â”‚   â””â”€â”€ scorer.py             # Weighted scoring calculation
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ experiment/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ runner.py             # Run 4x trials
â”‚   â”‚   â”œâ”€â”€ consistency.py        # Consistency analysis
â”‚   â”‚   â””â”€â”€ batch_processor.py    # Batch processing
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ accuracy.py           # Accuracy metrics
â”‚   â”‚   â”œâ”€â”€ agreement.py          # Fleiss' Kappa, Cohen's Kappa
â”‚   â”‚   â””â”€â”€ visualizer.py         # Plots & charts
â”‚   â”‚
â”‚   â””â”€â”€ ðŸ“ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py             # Logging system
â”‚       â”œâ”€â”€ data_loader.py        # Load/save data
â”‚       â””â”€â”€ statistics.py         # Statistical utilities
â”‚
â”œâ”€â”€ ðŸ“ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_experiment_analysis.ipynb
â”‚   â”œâ”€â”€ 03_statistical_tests.ipynb
â”‚   â””â”€â”€ 04_visualization.ipynb
â”‚
â”œâ”€â”€ ðŸ“ tests/
â”‚   â”œâ”€â”€ test_agents.py
â”‚   â”œâ”€â”€ test_rubric.py
â”‚   â”œâ”€â”€ test_scorer.py
â”‚   â””â”€â”€ test_metrics.py
â”‚
â”œâ”€â”€ ðŸ“ scripts/
â”‚   â”œâ”€â”€ setup_environment.py      # Initial setup
â”‚   â”œâ”€â”€ run_experiment.py         # Main experiment runner
â”‚   â”œâ”€â”€ generate_report.py        # Auto-generate report
â”‚   â””â”€â”€ export_for_paper.py       # Export data for publication
â”‚
â”œâ”€â”€ ðŸ“ docs/
â”‚   â”œâ”€â”€ API.md                    # API documentation
â”‚   â”œâ”€â”€ METHODOLOGY.md            # Research methodology
â”‚   â””â”€â”€ RESULTS_TEMPLATE.md       # Template for paper
â”‚
â”œâ”€â”€ .env.example                  # Template for API keys
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â””â”€â”€ PROJECT_PLAN.md (this file)
```

---

## ðŸš€ Implementation Roadmap

### Phase 1: Setup & Foundation (Week 1-2)
#### âœ… Checklist
- [ ] Setup Python environment (3.9+)
- [ ] Install dependencies (OpenAI, Google GenAI, pandas, numpy, scikit-learn, statsmodels)
- [ ] Setup API keys (OpenAI, Google Gemini)
- [ ] Create project structure
- [ ] Initialize Git repository
- [ ] Create default rubric configuration
- [ ] Setup logging system
- [ ] Write unit tests for core modules

**Deliverables:**
- âœ“ Working development environment
- âœ“ Core project structure
- âœ“ Configuration files

---

### Phase 2: Core Development (Week 3-4)
#### âœ… Checklist

**2.1 Rubric System**
- [ ] Implement `Rubric` class
- [ ] Default rubric loader
- [ ] Custom rubric validator
- [ ] Dynamic weighting system
- [ ] Export/Import rubric JSON

**2.2 Agent Development**
- [ ] `BaseAgent` abstract class
- [ ] ChatGPT Agent implementation
  - [ ] API integration
  - [ ] Prompt engineering for rubric-based grading
  - [ ] Response parsing
  - [ ] Error handling & retry logic
- [ ] Gemini Agent implementation
  - [ ] API integration
  - [ ] Prompt adaptation
  - [ ] Response parsing
  - [ ] Error handling
- [ ] Test both agents with sample essays

**2.3 Scoring System**
- [ ] Grade to point conversion (Aâ†’4, Bâ†’3, etc.)
- [ ] Weighted scoring calculation
- [ ] Multi-criteria aggregation
- [ ] Result validation

**Deliverables:**
- âœ“ Working AI agents
- âœ“ Rubric management system
- âœ“ Scoring calculator

---

### Phase 3: Experiment Framework (Week 5-6)
#### âœ… Checklist

**3.1 Experiment Runner**
- [ ] Batch processing system
- [ ] 4x trial automation per model
- [ ] Progress tracking & logging
- [ ] Result serialization (JSON/CSV)
- [ ] Error recovery mechanism
- [ ] API rate limiting handler

**3.2 Data Management**
- [ ] Data loader for questions/answers
- [ ] Lecturer scores importer
- [ ] Data validation & cleaning
- [ ] Unified dataset generator
- [ ] Result storage structure

**3.3 Consistency Analysis**
- [ ] Standard Deviation calculator
- [ ] Coefficient of Variation
- [ ] ICC (Intraclass Correlation)
- [ ] Range analysis
- [ ] Per-criteria consistency report

**Deliverables:**
- âœ“ Automated experiment pipeline
- âœ“ Consistency metrics implementation

---

### Phase 4: Evaluation Metrics (Week 7-8)
#### âœ… Checklist

**4.1 Accuracy Metrics**
- [ ] Exact match accuracy
- [ ] Mean Absolute Error (MAE)
- [ ] Root Mean Square Error (RMSE)
- [ ] Precision/Recall/F1-Score (per grade)
- [ ] Confusion matrix generator
- [ ] Per-criteria accuracy breakdown

**4.2 Agreement Metrics**
- [ ] Fleiss' Kappa implementation
  - [ ] Multi-rater agreement (3 raters)
  - [ ] Per-criteria Fleiss' Kappa
  - [ ] Overall agreement score
- [ ] Cohen's Kappa (pairwise)
  - [ ] ChatGPT vs Dosen
  - [ ] Gemini vs Dosen
  - [ ] ChatGPT vs Gemini
- [ ] Agreement interpretation (slight/fair/moderate/substantial)

**4.3 Statistical Tests**
- [ ] ANOVA (compare models)
- [ ] Paired t-test (significance testing)
- [ ] Wilcoxon signed-rank test (non-parametric)
- [ ] Effect size calculation (Cohen's d)

**Deliverables:**
- âœ“ Complete evaluation toolkit
- âœ“ Statistical analysis functions

---

### Phase 5: Visualization & Reporting (Week 9)
#### âœ… Checklist

**5.1 Visualization**
- [ ] Consistency plots (box plots, violin plots)
- [ ] Confusion matrices heatmaps
- [ ] Agreement comparison charts
- [ ] Accuracy comparison bar charts
- [ ] Scatter plots (AI vs Dosen scores)
- [ ] Distribution plots per grade

**5.2 Automated Reporting**
- [ ] Summary statistics generator
- [ ] LaTeX table formatter
- [ ] Result export to CSV/Excel
- [ ] Publication-ready figures (high-res)

**Deliverables:**
- âœ“ Visualization library
- âœ“ Auto-report generator

---

### Phase 6: Data Collection & Experiment (Week 10-12)
#### âœ… Checklist

**6.1 Data Preparation**
- [ ] Collect 8 essay questions
- [ ] Gather 10 student answers per question (80 total)
- [ ] Obtain lecturer ground truth scores for all 80 essays
- [ ] Validate data quality
- [ ] Organize by student ID and question ID

**6.2 Run Experiments**
- [ ] Pilot test (5 essays) to validate pipeline
- [ ] Run ChatGPT trials (4x per essay)
- [ ] Run Gemini trials (4x per essay)
- [ ] Monitor API costs & performance
- [ ] Validate all results

**6.3 Quality Checks**
- [ ] Check for missing data
- [ ] Verify consistency metrics
- [ ] Validate scoring calculations
- [ ] Review outliers

**Deliverables:**
- âœ“ Complete experimental dataset
- âœ“ Raw results from all trials

---

### Phase 7: Analysis & Results (Week 13-14)
#### âœ… Checklist

**7.1 Statistical Analysis**
- [ ] Calculate all metrics
- [ ] Perform significance tests
- [ ] Generate comparison tables
- [ ] Interpret Fleiss' Kappa results
- [ ] Identify patterns & insights

**7.2 Jupyter Notebook Analysis**
- [ ] Data exploration notebook
- [ ] Statistical tests notebook
- [ ] Visualization notebook
- [ ] Final results notebook

**7.3 Results Interpretation**
- [ ] Answer RQ1 (Consistency)
- [ ] Answer RQ2 (Accuracy)
- [ ] Answer RQ3 (Agreement)
- [ ] Answer RQ4 (Best Model)

**Deliverables:**
- âœ“ Complete statistical analysis
- âœ“ Interpreted results
- âœ“ Publication-ready tables & figures

---

### Phase 8: Paper Writing (Week 15-18)
#### âœ… Checklist

**8.1 Paper Structure (IMRaD)**
- [ ] Abstract
- [ ] Introduction
  - [ ] Background
  - [ ] Problem statement
  - [ ] Research questions
  - [ ] Contribution
- [ ] Related Work
  - [ ] AES systems
  - [ ] LLM for education
  - [ ] Inter-rater agreement studies
- [ ] Methodology
  - [ ] System architecture
  - [ ] Rubric design
  - [ ] Experiment design
  - [ ] Evaluation metrics
- [ ] Results
  - [ ] Consistency analysis
  - [ ] Accuracy comparison
  - [ ] Fleiss' Kappa results
  - [ ] Statistical significance
- [ ] Discussion
  - [ ] Interpretation
  - [ ] Implications
  - [ ] Limitations
- [ ] Conclusion & Future Work
- [ ] References

**8.2 Submission Preparation**
- [ ] Format to journal template
- [ ] Proofread & edit
- [ ] Co-author review
- [ ] Ethics & reproducibility statement
- [ ] Data availability statement
- [ ] Code repository link (GitHub)

**Deliverables:**
- âœ“ Complete manuscript
- âœ“ Supplementary materials

---

## ðŸ“¦ Dependencies

### Core Libraries
```
openai>=1.0.0
google-generativeai>=0.3.0
python-dotenv>=1.0.0
pydantic>=2.0.0
```

### Data Processing
```
pandas>=2.0.0
numpy>=1.24.0
```

### Machine Learning & Statistics
```
scikit-learn>=1.3.0
scipy>=1.11.0
statsmodels>=0.14.0
```

### Visualization
```
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.14.0
```

### Testing
```
pytest>=7.4.0
pytest-cov>=4.1.0
```

### Jupyter
```
jupyter>=1.0.0
ipykernel>=6.25.0
```

---

## ðŸŽ“ Expected Contributions for Q1 Paper

### Novel Contributions:
1. **Comparative study** ChatGPT vs Gemini untuk essay scoring (belum banyak research)
2. **Multi-criteria rubric-based** dengan weighted scoring
3. **Consistency analysis** melalui multiple trials (metodologi robust)
4. **Fleiss' Kappa** untuk multi-rater agreement (rigorous statistical method)
5. **Real-world educational dataset** dengan ground truth dari dosen

### Target Journals (Q1 Scopus):
- IEEE Transactions on Learning Technologies
- Computers & Education
- Educational Technology & Society
- Journal of Educational Computing Research
- International Journal of Artificial Intelligence in Education

---

## ðŸ”¬ Quality Assurance

### Code Quality
- [ ] Type hints di semua functions
- [ ] Docstrings (Google style)
- [ ] Unit tests (>80% coverage)
- [ ] Integration tests
- [ ] Code review checklist
- [ ] PEP 8 compliance

### Research Quality
- [ ] Reproducibility: Random seed fixed
- [ ] Documentation lengkap
- [ ] Version control (Git)
- [ ] Data versioning
- [ ] Experiment logging
- [ ] Ethics approval (jika perlu)

---

## ðŸ“ˆ Success Criteria

### Technical Success:
- âœ… System dapat grading 1500 essays dalam <24 jam
- âœ… API error rate <5%
- âœ… Consistency CV <15%
- âœ… Code coverage >80%

### Research Success:
- âœ… Fleiss' Kappa >0.60 (substantial agreement)
- âœ… Accuracy >75% (exact match)
- âœ… Statistical significance p<0.05
- âœ… Novel insights untuk publikasi

---

## ðŸ“ Notes

### API Cost Estimation (approx):
- ChatGPT (GPT-4o): ~$0.015-0.03 per essay Ã— 4 trials Ã— 80 essays = **$5-10**
- Gemini (2.0 Flash): ~$0.00025 per essay Ã— 4 trials Ã— 80 essays = **$0.08**
- **Total:** ~$5-15 (sangat terjangkau!)

### Timeline Risk:
- **API downtime:** Use retry logic & backup keys
- **Data collection delay:** Start early, work with lecturers
- **Analysis complexity:** Use established libraries

---

## ðŸ¤ Team Roles (if applicable)

- **Researcher/Developer:** System development, experimentation
- **Domain Expert:** Rubric validation, result interpretation
- **Statistician:** Statistical analysis validation
- **Writer:** Paper writing & revision

---

## ðŸ“š References to Study

1. Ke, Z., & Ng, V. (2019). Automated essay scoring: A survey of the state of the art. IJCAI.
2. Hussein, M. A., et al. (2019). Automated language essay scoring systems: a literature review. PeerJ Computer Science.
3. Fleiss, J. L. (1971). Measuring nominal scale agreement among many raters. Psychological bulletin.
4. Landis, J. R., & Koch, G. G. (1977). The measurement of observer agreement for categorical data. Biometrics.

---

**Last Updated:** December 10, 2025  
**Version:** 1.0  
**Status:** Planning Phase
