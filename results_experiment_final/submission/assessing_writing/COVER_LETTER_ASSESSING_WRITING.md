# Cover Letter for Assessing Writing

**To:** Professor [Editor Name]  
**Editor-in-Chief, Assessing Writing**  
**Date:** December 15, 2025

---

Dear Professor [Editor Name],

We are pleased to submit our original research manuscript titled **"Comparative Evaluation of ChatGPT-4o and Gemini-2.5-Flash for Automated Essay Scoring: A Multi-Strategy Analysis of Reliability, Validity, and Practical Implications"** for consideration for publication in *Assessing Writing*.

## Relevance to *Assessing Writing*

This manuscript directly addresses *Assessing Writing*'s core focus on writing assessment methods and technologies by providing rigorous empirical evaluation of large language models (LLMs) for automated essay scoring. As LLM-based assessment tools become increasingly prevalent in educational settings, our study offers evidence-based guidance for their responsible deployment in writing assessment contexts.

Our research aligns with the journal's emphasis on:

1. **Assessment Validity and Reliability:** We provide comprehensive psychometric evaluation including agreement with expert human grading (validity) and consistency across repeated assessments (reliability) using established metrics (Quadratic Weighted Kappa, Intraclass Correlation, Fleiss' Kappa).

2. **Alternative Assessment Technologies:** Our systematic comparison of two state-of-the-art LLMs (ChatGPT-4o and Gemini-2.5-Flash) across three prompting strategies represents a methodological advance in understanding how design choices impact assessment quality.

3. **Practical Implementation:** We develop evidence-based protocols for hybrid assessment that strategically combine automated and human grading, addressing cost-benefit trade-offs critical for writing program administrators.

4. **International Perspectives:** Our study in Indonesian higher education context contributes to understanding LLM-based writing assessment beyond English-dominant settings, addressing cross-cultural validity concerns.

## Research Significance

### Novel Contributions to Writing Assessment Literature

1. **Comprehensive Psychometric Evaluation:** With 4,473 grading instances across 10 independent trials per configuration, we provide the most extensive reliability analysis of LLM-based essay scoring to date, revealing critical consistency concerns (e.g., Gemini Few-shot κ=0.346, "fair agreement" only) that would not be detected in single-trial designs.

2. **Confusion Matrix Analysis for Writing Assessment:** We introduce detailed grade-level performance analysis showing LLMs achieve near-zero recall (F1≈0.000) for high-quality essays (Grades 4-5), a critical limitation for writing assessment that has not been previously documented.

3. **Systematic Bias Detection:** Our findings reveal that lenient prompting introduces 45-55% over-grading, making such strategies wholly inappropriate for summative writing assessment despite apparent reliability—a crucial finding for practitioners.

4. **Evidence-Based Implementation Protocols:** We develop tiered grading frameworks that reduce assessment costs by 77% while maintaining quality standards through strategic allocation of human review.

### Key Findings Relevant to Writing Assessment Practice

- **ChatGPT zero-shot achieves substantial agreement with expert raters** (QWK=0.600, 62.4% exact agreement, 92.8% adjacent agreement) with excellent consistency (ICC=0.969, Fleiss' κ=0.838), comparable to human inter-rater reliability reported in writing assessment literature.

- **Prompting strategy dramatically impacts assessment quality:** Few-shot approaches, often advocated in natural language processing literature, paradoxically *reduce* reliability for Gemini (κ=0.346 vs 0.530 for zero-shot), challenging assumptions about optimal LLM configuration for writing assessment.

- **Grade-dependent performance requires mandatory human review:** LLMs demonstrate severe limitations in identifying high-quality writing (F1≈0 for Grades 4-5), necessitating hybrid protocols rather than full automation—a critical insight for maintaining assessment validity.

- **Cost-effectiveness enables broader access:** Gemini's 34× cost advantage over ChatGPT ($6.40 vs $220 annually for 10,000 essays) makes automated formative feedback financially viable for resource-constrained writing programs while ChatGPT remains optimal for high-stakes summative assessment.

## Methodological Strengths

Our study addresses methodological gaps in prior automated writing evaluation research through:

1. **Multi-trial reliability assessment** (10 independent trials) rather than single assessments common in prior work
2. **Comparative experimental design** evaluating two LLMs across three strategies simultaneously
3. **Comprehensive error analysis** with severity classification and confusion matrices
4. **Real-world writing samples** from authentic capstone projects rather than contrived prompts
5. **Complete transparency** with reproducible code, supplementary data, and detailed statistical outputs

## Alignment with Recent Journal Publications

Our manuscript complements recent *Assessing Writing* publications on:

- Automated writing evaluation systems and their validity (aligning with calls for rigorous psychometric evaluation)
- Fairness and bias in automated scoring (we document systematic over-grading bias)
- Teacher and student perspectives on AWE (we provide empirical evidence to inform such discussions)
- Alternative assessment technologies in writing programs (we offer practical implementation frameworks)

## Practical Impact

This research provides actionable guidance for:

- **Writing program administrators** making technology adoption decisions
- **Assessment researchers** evaluating automated scoring systems
- **Instructors** using LLM feedback for formative assessment
- **Policymakers** developing guidelines for AI in educational assessment

Our tiered grading protocols and decision trees enable evidence-based implementation rather than blanket adoption or rejection of LLM-based assessment.

## Manuscript Specifications

- **Word Count:** 7,959 words (within journal limits of 7,000-10,000)
- **Article Type:** Full Research Article (empirical study)
- **Supplementary Materials:** Five supplementary documents with raw data, statistical outputs, extended tables, and reproducible code
- **Data Availability:** Complete dataset and analysis scripts available in public repository
- **Ethical Approval:** Obtained from [Institution] IRB

## Declaration Statements

- **Funding:** This research received no specific grant from funding agencies in the public, commercial, or not-for-profit sectors.
- **Conflicts of Interest:** The authors declare no conflicts of interest.
- **Data Availability:** All data and code are available at [repository link] under open license.
- **Ethical Compliance:** This study was approved by [Institution] Ethics Committee (Protocol #[XXX]). All participants provided informed consent.

## Why *Assessing Writing* is the Optimal Venue

*Assessing Writing* is uniquely positioned to disseminate our findings because:

1. **Specialized Audience:** Reaches researchers and practitioners specifically focused on writing assessment rather than general educational technology
2. **Methodological Standards:** Values rigorous psychometric evaluation and validity evidence central to our study
3. **Practical Orientation:** Emphasizes actionable implications for assessment practice, which our hybrid protocols provide
4. **International Scope:** Welcomes diverse cultural contexts, including our Indonesian higher education setting
5. **Technology Focus:** Actively publishes research on automated writing evaluation and emerging assessment technologies

## Suggested Reviewers

We suggest the following reviewers with expertise in automated writing evaluation and educational assessment:

1. **[Name 1], [Institution]** (email) - Expert in automated essay scoring and validity
2. **[Name 2], [Institution]** (email) - Specialist in writing assessment reliability
3. **[Name 3], [Institution]** (email) - Researcher in AI applications for writing assessment

## Conclusion

We believe this manuscript makes significant contributions to writing assessment research through rigorous empirical evaluation of LLM-based scoring, identification of critical limitations requiring human oversight, and development of practical implementation protocols. Our findings challenge common assumptions about optimal LLM configuration while providing evidence-based frameworks for responsible integration of these technologies in writing assessment contexts.

We confirm that this manuscript is original research, has not been published elsewhere, and is not under consideration by any other journal. All authors have approved the manuscript and agree with its submission to *Assessing Writing*.

Thank you for considering our manuscript. We look forward to your response and the opportunity to contribute to *Assessing Writing*'s mission of advancing knowledge in writing assessment methods and practices.

Sincerely,

**[Lead Author Name, PhD]**  
[Title/Position]  
[Institution]  
[Email]  
[ORCID: xxxx-xxxx-xxxx-xxxx]

**Co-Authors:**  
[Name 2], [Institution]  
[Name 3], [Institution]

---

**Corresponding Author:**  
[Name]  
[Full Address]  
Phone: [+XX XXX XXX XXXX]  
Email: [email]

**Manuscript Statistics:**
- Title: 139 characters
- Abstract: 289 words
- Main text: 7,959 words
- Tables: 5
- Figures: 1 (graphical abstract)
- Supplementary files: 5
- References: [XX] (to be confirmed)
