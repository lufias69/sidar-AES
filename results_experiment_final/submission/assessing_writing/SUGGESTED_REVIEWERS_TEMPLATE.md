# Suggested Reviewers for Assessing Writing Submission

**Manuscript:** Comparative Evaluation of ChatGPT-4o and Gemini-2.5-Flash for Automated Essay Scoring

---

## Selection Criteria

✅ Expertise in automated writing evaluation/assessment  
✅ Published in *Assessing Writing* or related journals  
✅ International diversity (different countries/institutions)  
✅ No conflicts of interest (not co-authors, collaborators, same institution)  
✅ Active researchers (publications in last 5 years)

---

## Suggested Reviewers

### Reviewer 1: Expert in Automated Essay Scoring Validity

**Dr. [Name to be added]**  
Professor of Educational Measurement  
[Institution Name]  
[Country]  
Email: [email@institution.edu]

**Expertise:**
- Automated essay scoring systems
- Validity studies in writing assessment
- Psychometric evaluation of AES
- Machine learning for educational assessment

**Relevant Publications:**
1. [Author et al.] (2023). Validity evidence for automated writing evaluation systems. *Assessing Writing*, *XX*(X), XXX-XXX. https://doi.org/xxx
2. [Author et al.] (2022). Reliability and fairness in automated essay scoring. *Journal of Educational Measurement*, *XX*(X), XXX-XXX.

**Why Suggested:**
Leading international expert in validity studies for automated essay scoring. Has published extensively on psychometric evaluation of AES systems, directly relevant to our multi-dimensional reliability and validity analysis (RQ1-RQ2).

**No Conflicts:** Not a collaborator, different institution, no prior working relationship.

---

### Reviewer 2: Expert in Writing Assessment Reliability

**Dr. [Name to be added]**  
Associate Professor of Applied Linguistics  
[Institution Name]  
[Country]  
Email: [email@institution.edu]

**Expertise:**
- Writing assessment methodology
- Inter-rater reliability in essay scoring
- Rubric development and validation
- Second language writing assessment

**Relevant Publications:**
1. [Author et al.] (2024). Multi-rater agreement in writing assessment: Beyond kappa. *Assessing Writing*, *XX*(X), XXX-XXX. https://doi.org/xxx
2. [Author et al.] (2023). Reliability of analytic scoring rubrics for academic writing. *Language Testing*, *XX*(X), XXX-XXX.

**Why Suggested:**
Expertise in reliability analysis for writing assessment aligns perfectly with our 10-trial consistency evaluation using ICC, Fleiss' κ, and Cronbach's α (RQ2). Experience with multi-rater designs will be valuable for reviewing our methodology.

**No Conflicts:** Independent researcher, no previous collaboration.

---

### Reviewer 3: Expert in AI/LLM for Educational Assessment

**Dr. [Name to be added]**  
Assistant Professor of Learning Analytics  
[Institution Name]  
[Country]  
Email: [email@institution.edu]

**Expertise:**
- Large language models in education
- AI-assisted assessment systems
- Natural language processing for writing
- Educational technology evaluation

**Relevant Publications:**
1. [Author et al.] (2024). ChatGPT for formative assessment: Opportunities and challenges. *Computers & Education*, *XX*(X), XXX-XXX. https://doi.org/xxx
2. [Author et al.] (2023). Evaluating LLM performance on educational tasks. *British Journal of Educational Technology*, *XX*(X), XXX-XXX.

**Why Suggested:**
Technical expertise in LLM evaluation will provide valuable perspective on our comparative analysis of ChatGPT-4o and Gemini-2.5-Flash across prompting strategies (RQ3). Recent publications on LLM in education demonstrate active research in this emerging area.

**No Conflicts:** Different research group, no institutional overlap.

---

### Reviewer 4: Expert in Writing Program Administration & Assessment Implementation

**Dr. [Name to be added]**  
Director of Writing Programs / Professor  
[Institution Name]  
[Country]  
Email: [email@institution.edu]

**Expertise:**
- Writing program assessment
- Large-scale writing assessment
- Assessment implementation and policy
- Cost-benefit analysis in education

**Relevant Publications:**
1. [Author et al.] (2023). Implementing automated feedback in writing programs: Lessons learned. *Assessing Writing*, *XX*(X), XXX-XXX. https://doi.org/xxx
2. [Author et al.] (2022). Cost-effective writing assessment at scale. *WPA: Writing Program Administration*, *XX*(X), XXX-XXX.

**Why Suggested:**
Practical expertise in writing program administration will provide valuable perspective on our cost-benefit analysis and hybrid grading protocols (RQ5). Can evaluate feasibility and actionability of our recommendations for real-world implementation.

**No Conflicts:** Practitioner-researcher perspective, no conflicts.

---

### Reviewer 5 (Optional): Expert in Cross-Cultural Assessment

**Dr. [Name to be added]**  
Professor of International Education  
[Institution Name]  
[Country]  
Email: [email@institution.edu]

**Expertise:**
- Cross-cultural writing assessment
- Assessment in non-English contexts
- International writing pedagogy
- Validity in diverse educational settings

**Relevant Publications:**
1. [Author et al.] (2023). Writing assessment beyond English: International perspectives. *Journal of Second Language Writing*, *XX*(X), XXX-XXX. https://doi.org/xxx
2. [Author et al.] (2022). Cultural validity in automated writing evaluation. *Assessing Writing*, *XX*(X), XXX-XXX.

**Why Suggested:**
Expertise in non-English context assessment valuable for evaluating our Indonesian higher education study. Can provide perspective on generalizability concerns and cross-cultural validity (Section 4.4.1).

**No Conflicts:** Different geographic region, independent researcher.

---

## How to Find Actual Reviewers

### Step 1: Search Recent Publications

**Databases:**
- Google Scholar: https://scholar.google.com
- Web of Science
- Scopus
- ERIC (education specific)

**Search Terms:**
- "automated essay scoring" + "validity"
- "automated writing evaluation" + "reliability"
- "ChatGPT" + "assessment" + "education"
- "large language model" + "writing assessment"
- "inter-rater reliability" + "essay scoring"

**Target Journals:**
- *Assessing Writing* (last 5 years)
- *Language Testing*
- *Educational Technology Research and Development*
- *Computers & Education*
- *British Journal of Educational Technology*

### Step 2: Check Author Profiles

For each potential reviewer:
1. **Google Scholar Profile:** Check recent publications, h-index, research interests
2. **University Website:** Verify current affiliation and position
3. **ORCID Profile:** Check research outputs and collaborations
4. **LinkedIn/ResearchGate:** Verify active in field

### Step 3: Verify No Conflicts

❌ **Exclude if:**
- Same institution as any author
- Recent co-author (last 3 years)
- PhD advisor/advisee relationship
- Current collaborator
- Close personal relationship
- Competing research group

✅ **Good indicators:**
- Published in target journal (*Assessing Writing*)
- Recent activity (2023-2025 publications)
- International (not all from same country)
- Mix of senior and early-career researchers
- Diverse methodological perspectives

### Step 4: Get Contact Information

- Usually on university department website
- Email format: firstname.lastname@university.edu
- ORCID profile often has email
- Previous publications (corresponding author emails)

---

## Example Search Strategy

### For Reviewer 1 (AES Validity Expert):

**Google Scholar Search:**
```
"automated essay scoring" AND "validity" AND "Assessing Writing"
```

**Recent relevant papers in Assessing Writing (2023-2025):**
1. Look at authors of papers on AES validity
2. Check their recent work
3. Verify they're active researchers
4. Get university affiliation
5. Find email on department website

**Example Profile Check:**
- ✅ Published 3+ papers on AES in last 5 years
- ✅ Currently at [University], Professor level
- ✅ No connection to our research team
- ✅ International (different country)
- ✅ Email available on faculty page

---

## Template for Editorial Manager

Once you identify reviewers, enter in this format:

```
Title: Professor / Dr.
First Name: [FirstName]
Last Name: [LastName]
Email: [email@institution.edu]
Institution: [Full Institution Name]
Department: [Department Name]
City: [City]
Country: [Country]

Expertise: [One sentence, e.g., "Expert in automated essay scoring validity and reliability"]

Suggested because: [2-3 sentences explaining why this reviewer is appropriate for this manuscript]

Publications relevant to this manuscript:
1. [Citation 1]
2. [Citation 2]
```

---

## Recommended Distribution

**For best review outcome, suggest:**
- 1-2 psychometric/measurement experts (validity/reliability focus)
- 1-2 AI/LLM experts (technical evaluation)
- 1 writing assessment practitioner (implementation perspective)
- Ensure international diversity (3+ countries)
- Mix of senior (Professor) and mid-career (Associate/Assistant Professor)

---

## Notes for Submission

**In Editorial Manager:**
- You can suggest 3-5 reviewers (recommended: 4)
- Editor not required to use your suggestions
- But helpful suggestions increase chances of appropriate reviewers
- Do NOT suggest anyone with potential conflicts
- Brief explanation (2-3 sentences) for each is helpful

**What Editor Looks For:**
- Reviewers with clear expertise match
- International diversity
- No obvious conflicts
- Active researchers (recent publications)
- Specific rationale for suggestion

---

## Action Items

- [ ] Search Google Scholar for recent *Assessing Writing* papers on AES
- [ ] Identify 4-5 potential reviewers from different institutions/countries
- [ ] Verify each has relevant expertise and publications
- [ ] Check for any conflicts of interest
- [ ] Find current email addresses
- [ ] Fill in TITLE_PAGE.md "Suggested Reviewers" section
- [ ] Prepare brief rationale for each suggestion

**Time Estimate:** 30-45 minutes

**Priority:** Medium (optional but recommended - improves review quality)

---

**Last Updated:** December 15, 2025  
**Status:** Template - needs specific names/contacts to be added
